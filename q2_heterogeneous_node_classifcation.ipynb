{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Node Classification in a Heterogeneous Graph\n",
    "\n",
    "In this question, you will explore node classifcation using GNNs in a heterogeneous graph with two types of nodes containing different number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a Seed for Reproducibility\n",
    "\n",
    "To ensure that our results are reproducible, we will set a fixed seed for the random number generator. This step is crucial for educational and testing purposes, as it allows the same random numbers to be generated each time the notebook is run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torch.optim import lr_scheduler\n",
    "import networkx as nx\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_PATH = \"data/q2_train.json\"\n",
    "VAL_PATH = \"data/q2_val.json\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 Dataset\n",
    "\n",
    "The heterogeneous graph $G=(V,E)$ consists of a set of nodes $V$ with two types of nodes:\n",
    "- **Type 1 nodes**: Feature dimension of $d_1 = 20$\n",
    "- **Type 2 nodes**: Feature dimension of $d_2 = 30$\n",
    "\n",
    "Each node belongs to one of two classes:\n",
    "  - **Class 0**: $y_i = 0$\n",
    "  - **Class 1**: $y_i = 1$\n",
    "\n",
    "In total, there are $N=N_1 + N_2$ nodes in the graph.\n",
    "\n",
    "In the following cell, we load the data from the given **JSON** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_json(json_path):\n",
    "    \"\"\"\n",
    "    Loads a JSON describing a heterogeneous graph\n",
    "\n",
    "    Returns:\n",
    "      A : torch.FloatTensor, shape (N, N)\n",
    "          Adjacency matrix for all nodes (0...N-1).\n",
    "      X1 : torch.FloatTensor, shape (N1, d1)\n",
    "      X2 : torch.FloatTensor, shape (N2, d2)\n",
    "      y : torch.LongTensor, shape (N,)\n",
    "          Labels for each node (0 or 1 for binary).\n",
    "      mapping : dict\n",
    "          mapping[i] = {\n",
    "             \"node_type\": str (\"type1\" or \"type2\"),\n",
    "             \"feat_index\": int index into the relevant feature array\n",
    "          }\n",
    "        This tells us how adjacency row i corresponds to a row in either type1_features or type2_features.\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    nodes_info = data[\"nodes\"]\n",
    "    # Sort so adjacency row i matches global_index = i\n",
    "    nodes_info.sort(key=lambda x: x[\"global_index\"])\n",
    "\n",
    "    N = len(nodes_info)\n",
    "    A = torch.zeros((N, N), dtype=torch.float)\n",
    "\n",
    "    # We'll store raw lists for type1 and type2, then convert to tensors\n",
    "    type1_list = []\n",
    "    type2_list = []\n",
    "    # We'll also store the mapping from adjacency index -> (type, index_in_that_type)\n",
    "    mapping = {}\n",
    "    # We'll store labels in a list\n",
    "    labels_list = []\n",
    "\n",
    "    # Step 1: first pass to build adjacency\n",
    "    #         (we'll also note how many type1 vs type2)\n",
    "    for i, node in enumerate(nodes_info):\n",
    "        for nbr_idx in node[\"connected_nodes\"]:\n",
    "            A[i, nbr_idx] = 1.0\n",
    "            A[nbr_idx, i] = 1.0  # undirected\n",
    "\n",
    "    # Step 2: second pass, fill feature arrays, mapping\n",
    "    type1_count = 0\n",
    "    type2_count = 0\n",
    "\n",
    "    for i, node in enumerate(nodes_info):\n",
    "        node_type = node[\"node_type\"]\n",
    "        feats = node[\"features\"]\n",
    "        label = node[\"label\"] if node[\"label\"] is not None else 0\n",
    "        labels_list.append(label)\n",
    "\n",
    "        if node_type == \"type1\":\n",
    "            type1_list.append(feats)\n",
    "            mapping[i] = {\"node_type\": \"type1\", \"feat_index\": type1_count}\n",
    "            type1_count += 1\n",
    "        else:\n",
    "            type2_list.append(feats)\n",
    "            mapping[i] = {\"node_type\": \"type2\", \"feat_index\": type2_count}\n",
    "            type2_count += 1\n",
    "\n",
    "    # Convert type1 and type2 lists into float tensors\n",
    "    X1 = torch.tensor(type1_list, dtype=torch.float) if len(type1_list) else torch.empty((0,0))\n",
    "    X2 = torch.tensor(type2_list, dtype=torch.float) if len(type2_list) else torch.empty((0,0))\n",
    "\n",
    "    # Convert labels\n",
    "    y = torch.tensor(labels_list, dtype=torch.long)  # shape (N,)\n",
    "\n",
    "    return A, X1, X2, y, mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we load the **train** and **validation** datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train, X1_train, X2_train, y_train, mapping_train = load_data_from_json(TRAIN_PATH)\n",
    "A_val,   X1_val,   X2_val,   y_val,   mapping_val   = load_data_from_json(VAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.1 Interpretation of the Dataset\n",
    "\n",
    "Before diving into the modeling phase, it's crucial to explore and understand the data. Immplement the following functions to assist in this analysis:\n",
    "\n",
    "\n",
    "### Question 2.1.c `plot_graph`\n",
    "- Implement the `plot_graph` function which visualizes the graph structure.\n",
    "- Color nodes by class labels and display their types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "\n",
    "def plot_graph():\n",
    "    \"\"\"\n",
    "    Visualize the graph structure with:\n",
    "      - node colors based on their labels\n",
    "      - node types written over the nodes\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 2.1.d `plot_node_feature_distributions`\n",
    "Implement the `plot_node_feature_distributions` function.\n",
    "\n",
    "- This function should plot the distribution of the node features.\n",
    "- Do not consider the distribution of a specific feature $x_i$; instead, consider the mean of the feature vector $\\mathbf{x}$ for each node.\n",
    "- All distributions should appear on the **same figure**, so you can compare the feature distributions of $t_1$ and $t_2$ nodes as well as $c_1$ and $c_2$ nodes in Question 2.1.e\n",
    "- Check Figure 1 in the Coursework Description PDF for reference on how your plot should look.\n",
    "\n",
    "\n",
    "**NOTE:** The questions above are named as Question 2.1.c and Question 2.1.d by purpose to be consistent with the Coursework Description PDF, the other subquestions of Question 2 need to be answered in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "\n",
    "\n",
    "def plot_node_feature_distributions():\n",
    "    \"\"\"\n",
    "    Plots the distributions (as normal PDFs) of the *average* feature values\n",
    "    for four groups of nodes on the *same* figure:\n",
    "      * type1, class=1\n",
    "      * type1, class=2\n",
    "      * type2, class=1\n",
    "      * type2, class=2\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call your functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "plot_graph()\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "plot_node_feature_distributions()\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2 Naive Solution\n",
    "\n",
    "Nothing to implement in Question 2.2 :)\n",
    "\n",
    "In the naive solution, we handle the challenge of heterogeneous node feature dimensions by zero-padding all node feature vectors to match the maximum dimension. This ensures uniformity in the input dimensions for the Graph Convolutional Network (GCN).\n",
    "\n",
    "### Zero Padding\n",
    "Let the feature dimension of a node $i$ be $d_i$. To create uniform dimensions:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i^{\\text{padded}} = \\begin{bmatrix}\n",
    "\\mathbf{x}_i \\\\\n",
    "\\mathbf{0}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $ \\mathbf{x}_i \\in \\mathbb{R}^{d_i} $ is the original feature vector, and the padding $ \\mathbf{0} $ is of size $ d_{\\text{max}} - d_i $. Here, $ d_{\\text{max}} = \\max\\{d_i \\mid i \\in \\text{nodes}\\} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(A, type1_features, type2_features, mapping):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      A : (N, N) adjacency\n",
    "      type1_features : (num_type1, d1)\n",
    "      type2_features : (num_type2, d2)\n",
    "      mapping : a dict so that mapping[i] = {\"node_type\": \"type1\" or \"type2\",\n",
    "                                             \"feat_index\": index in that type's feature array}\n",
    "\n",
    "    Produces:\n",
    "      X : torch.FloatTensor, shape (N, d_max)\n",
    "        Where d_max = max(d1, d2).\n",
    "        For each node i, we copy either type1_features or type2_features \n",
    "        into row i, zero-padding if needed.\n",
    "      d_max : the dimension used for the padding.\n",
    "    \"\"\"\n",
    "    device = A.device\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Determine the dimensions of type1 & type2\n",
    "    d1 = type1_features.shape[1] if type1_features.ndim == 2 and type1_features.shape[0] > 0 else 0\n",
    "    d2 = type2_features.shape[1] if type2_features.ndim == 2 and type2_features.shape[0] > 0 else 0\n",
    "\n",
    "    d_max = max(d1, d2)\n",
    "\n",
    "    # Create a (N, d_max) zero tensor\n",
    "    X = torch.zeros((N, d_max), dtype=torch.float, device=device)\n",
    "\n",
    "    # Fill each row\n",
    "    for i in range(N):\n",
    "        node_type = mapping[i][\"node_type\"]\n",
    "        feat_index = mapping[i][\"feat_index\"]\n",
    "        if node_type == \"type1\":\n",
    "            # If we have any type1 features at all\n",
    "            if d1 > 0:\n",
    "                X[i, :d1] = type1_features[feat_index].to(device)\n",
    "        else:\n",
    "            # type2\n",
    "            if d2 > 0:\n",
    "                X[i, :d2] = type2_features[feat_index].to(device)\n",
    "\n",
    "    return X, d_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network (GCN)\n",
    "\n",
    "We are going to use a 2-layer GCN for the **binary node classification** task on the heterogeneous graph. Below, you can see the mathematical representation to generate predictions from GCN.\n",
    "\n",
    "The feature update rule for the next layer $ H_{k+1} $ in a graph convolutional network is given by the equation\n",
    "\n",
    "$$\n",
    "H_{k+1} = \\sigma(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H_k \\Omega_k + \\Beta_k)\n",
    "$$\n",
    "\n",
    "where  $\\tilde{D}$ is the degree matrix with added self-loops, $\\tilde{A}$ is the adjacency matrix with self-loops, $H_k$ are the features from the previous layer, $ \\Omega_k $ is the weight matrix at layer $ k $, $\\Beta_k$ is the bias vector at layer $k$, and $\\sigma $ denotes the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ is the normalized adjacency matrix with self-loops. Below, you need to implement this normalization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return D_inv @ A_tilde @ D_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_train_self_loops = A_train + torch.eye(A_train.size(0))\n",
    "A_val_self_loops = A_val + torch.eye(A_val.size(0))\n",
    "\n",
    "A_train_normalized = symmetric_normalize(A_train_self_loops)\n",
    "A_val_normalized = symmetric_normalize(A_val_self_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we define a simple 2-layer GCN model to use with our padded node features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of a Graph Convolutional Network (GCN).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.use_nonlinearity = use_nonlinearity\n",
    "        self.Omega = nn.Parameter(torch.randn(input_dim, output_dim) * torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim)))\n",
    "        self.beta = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, H_k, A_normalized):\n",
    "        agg = torch.matmul(A_normalized, H_k) # local agg\n",
    "        H_k_next = torch.matmul(agg, self.Omega) + self.beta\n",
    "        return F.relu(H_k_next) if self.use_nonlinearity else H_k_next\n",
    "\n",
    "class GraphNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Graph Neural Network model using two layers of Graph Convolutional Network (GCN)\n",
    "    for binary classification. The sigmoid activation is applied in the output layer only if\n",
    "    use_nonlinearity is set to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GraphNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.gcn1 = GCNLayer(input_dim, hidden_dim, True)\n",
    "        self.gcn2 = GCNLayer(hidden_dim, 1, False)\n",
    "\n",
    "    def forward(self, A, X, **kwargs):\n",
    "        # Pass through GCN layers\n",
    "        H1 = self.gcn1(X, A)\n",
    "        H2 = self.gcn2(H1, A)  # Output shape: (num_nodes, 1)\n",
    "\n",
    "        output = torch.sigmoid(H2)  # Sigmoid activation per node\n",
    "        if torch.isnan(output).any():\n",
    "            output = torch.where(torch.isnan(output), torch.zeros_like(output), output)\n",
    "\n",
    "        if kwargs.get(\"return_embeddings\", None):\n",
    "            return output, H1, H2\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do not modify the following `train_model` and `evaluate_model` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    y_train,\n",
    "    val_model_inputs,\n",
    "    y_val,\n",
    "    num_epochs=100,\n",
    "    lr=0.01,\n",
    "    validate_every=10,\n",
    "    patience=10,\n",
    "    **train_model_inputs\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validate every 'validate_every' epochs, and pick the \n",
    "    checkpoint with best validation accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The PyTorch model to train.\n",
    "    y_train : torch.Tensor\n",
    "        Training labels (shape: (N,)).\n",
    "    val_model_inputs : dict\n",
    "        Dictionary of keyword arguments for the model's forward pass \n",
    "        for the validation set.\n",
    "    y_val : torch.Tensor\n",
    "        Validation labels (shape: (N_val,)).\n",
    "    num_epochs : int\n",
    "        Number of training epochs.\n",
    "    lr : float\n",
    "        Learning rate for the optimizer.\n",
    "    validate_every : int\n",
    "        Validate (and possibly checkpoint) every 'validate_every' epochs.\n",
    "    **train_model_inputs :\n",
    "        Keyword arguments for the model's forward pass for the training set.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_loss_history : list\n",
    "        The training loss history across epochs.\n",
    "    best_model_state_dict : dict\n",
    "        The state dictionary of the model achieving the best validation accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=patience)\n",
    "    loss_history = []\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # Forward pass on training data\n",
    "        out = model(**train_model_inputs)  # shape: (N, 1)\n",
    "        out = out.squeeze(-1)  # shape: (N,)\n",
    "        y_float = y_train.float()  # for BCE\n",
    "\n",
    "        # Compute training loss\n",
    "        loss = binary_cross_entropy(out, y_float)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record training loss\n",
    "        loss_value = loss.item()\n",
    "        loss_history.append(loss_value)\n",
    "\n",
    "        # Validation step\n",
    "        if (epoch + 1) % validate_every == 0 or (epoch + 1) == num_epochs:\n",
    "            val_prec, val_rec, val_f1, _ = evaluate_model(model, y_val, **val_model_inputs)\n",
    "            scheduler.step(val_f1)\n",
    "            \n",
    "            lr = get_lr(optimizer)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                f\"Train Loss: {loss_value:.4f} - \"\n",
    "                f\"Val Precision: {val_prec:.4f} - \"\n",
    "                f\"Val Recall: {val_rec:.4f} - \"\n",
    "                f\"Val F1: {val_f1:.4f} - \"\n",
    "                f\"LR: {lr}\"\n",
    "            )\n",
    "    \n",
    "            # Check if this is the best f1 score so far\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "            if lr < 1e-5:\n",
    "                break\n",
    "\n",
    "    \n",
    "    # If we have a best model, load it\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    return loss_history, best_model_state_dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, y, **kwargs):\n",
    "    \"\"\"\n",
    "    Runs forward pass, calculates binary predictions (threshold=0.5),\n",
    "    and returns the accuracy score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    out = model(**kwargs)  # shape: (N, 1)\n",
    "    out = out.squeeze(-1)  # shape: (N,)\n",
    "    preds = (out >= 0.5).long()\n",
    "\n",
    "    y_true = y.cpu().numpy()\n",
    "    y_pred = preds.cpu().numpy()\n",
    "\n",
    "    return precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to plot the history of loss during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_history, title):\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history, label=\"loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(val_precs, val_recs, val_f1s, title_prefix='', figsize=(15,5)):\n",
    "    \"\"\"\n",
    "    Plots three bar charts comparing precision, recall, and F1 scores \n",
    "    for multiple models on a single figure, each sorted by its own metric.\n",
    "    The best model is always on the right for each plot.\n",
    "    \n",
    "    Args:\n",
    "        val_precs (dict): Dictionary of model -> precision score\n",
    "        val_recs (dict): Dictionary of model -> recall score\n",
    "        val_f1s (dict): Dictionary of model -> F1 score\n",
    "        title_prefix (str): Optional prefix for subplot titles\n",
    "        figsize (tuple): Size of the overall figure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the figure\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize)\n",
    "    \n",
    "    # --- Sort and plot Precision ---\n",
    "    # Sort by precision (ascending), so best is on the right\n",
    "    sorted_precs = sorted(val_precs.items(), key=lambda x: x[1])\n",
    "    prec_models = [x[0] for x in sorted_precs]  # model names\n",
    "    prec_scores = [x[1] for x in sorted_precs]  # scores\n",
    "    min_prec = min(prec_scores)\n",
    "    \n",
    "    axes[0].bar(prec_models, prec_scores, color='skyblue')\n",
    "    # Start the y-axis a bit lower than the min value\n",
    "    axes[0].set_ylim([\n",
    "        min_prec - 0.05*(1 if min_prec == 0 else min_prec), \n",
    "        max(1.0, max(prec_scores))  # in case some scores > 1.0\n",
    "    ])\n",
    "    axes[0].set_title(f'{title_prefix}Precision', fontsize=12)\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_xticks(range(len(prec_models)))\n",
    "    axes[0].set_xticklabels(prec_models, rotation=45, ha='right')\n",
    "    \n",
    "    # --- Sort and plot Recall ---\n",
    "    sorted_recs = sorted(val_recs.items(), key=lambda x: x[1])\n",
    "    rec_models = [x[0] for x in sorted_recs]\n",
    "    rec_scores = [x[1] for x in sorted_recs]\n",
    "    min_recs = min(rec_scores)\n",
    "    \n",
    "    axes[1].bar(rec_models, rec_scores, color='salmon')\n",
    "    axes[1].set_ylim([\n",
    "        min_recs - 0.05*(1 if min_recs == 0 else min_recs),\n",
    "        max(1.0, max(rec_scores))\n",
    "    ])\n",
    "    axes[1].set_title(f'{title_prefix}Recall', fontsize=12)\n",
    "    axes[1].set_xticks(range(len(rec_models)))\n",
    "    axes[1].set_xticklabels(rec_models, rotation=45, ha='right')\n",
    "    \n",
    "    # --- Sort and plot F1 ---\n",
    "    sorted_f1s = sorted(val_f1s.items(), key=lambda x: x[1])\n",
    "    f1_models = [x[0] for x in sorted_f1s]\n",
    "    f1_scores = [x[1] for x in sorted_f1s]\n",
    "    min_f1 = min(f1_scores)\n",
    "    \n",
    "    axes[2].bar(f1_models, f1_scores, color='limegreen')\n",
    "    axes[2].set_ylim([\n",
    "        min_f1 - 0.05*(1 if min_f1 == 0 else min_f1),\n",
    "        max(1.0, max(f1_scores))\n",
    "    ])\n",
    "    axes[2].set_title(f'{title_prefix}F1', fontsize=12)\n",
    "    axes[2].set_xticks(range(len(f1_models)))\n",
    "    axes[2].set_xticklabels(f1_models, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_precs, val_recs, val_f1s = {}, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Naively pad\n",
    "X_train, d_max_train = pad_features(A_train, X1_train, X2_train, mapping_train)\n",
    "X_val,   d_max_val   = pad_features(A_val,   X1_val,   X2_val,   mapping_val)\n",
    "\n",
    "print(\"Train adjacency:\", A_train.shape)\n",
    "print(\"Train features after padding:\", X_train.shape, \"with d_max =\", d_max_train)\n",
    "print(\"Val adjacency:\", A_val.shape)\n",
    "print(\"Val features after padding:\", X_val.shape, \"with d_max =\", d_max_val)\n",
    "\n",
    "# Convert labels to float if using BCE with sigmoid\n",
    "y_train = y_train.float()  # shape (N,)\n",
    "y_val   = y_val.float()\n",
    "\n",
    "# Build the GCN model\n",
    "input_dim = X_train.shape[1]  # the padded dimension\n",
    "hidden_dim = 2\n",
    "naive_gnn_model = GraphNeuralNetwork(input_dim, hidden_dim)\n",
    "\n",
    "# Train\n",
    "loss_history, _ = train_model(naive_gnn_model, y_train, val_model_inputs={\"A\":A_val_normalized.clone(), \"X\":X_val.clone()}, y_val=y_val.clone(), A=A_train_normalized, X=X_train, num_epochs=10000, lr=0.01, patience=50)\n",
    "\n",
    "# Test on validation data\n",
    "val_prec, val_rec, val_f1, _ = evaluate_model(naive_gnn_model, y_val, A=A_val_normalized, X=X_val)\n",
    "print(f\"[VAL] Prec={val_prec:.4f}, Rec={val_rec:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "val_precs[\"Naive GCN\"] = val_prec\n",
    "val_recs[\"Naive GCN\"] = val_rec\n",
    "val_f1s[\"Naive GCN\"] = val_f1\n",
    "\n",
    "# Plot loss\n",
    "plot_loss(loss_history, title=\"GCN Naive Approach Training Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3 Node-Type Aware GCN\n",
    "\n",
    "In this task, **design and implement** a node-type aware 2-layer GCN model that explicitly accounts for two distinct node types and performs binary node classification. Your design should adhere to the following requirements:\n",
    "\n",
    "- Consider two types of nodes without using padding.\n",
    "- Limit your model to **two layers**, as in the naive solution.\n",
    "- You may either use the provided `GCNLayer` and `GraphNeuralNetwork` classes or implement your own classes. However, the final model architecture must be implemented within the `HeteroGCN` class.\n",
    "- While you are allowed to introduce additional learnable parameters, your approach must **explicitly** account for different node types rather than merely increasing the number of learnable parameters.\n",
    "- Determine the optimal hyperparameters for your model, such as hidden dimensions, learning rate, scheduler patience, etc. These hyperparameters do not need to match those of the naive model, as each design has its own optimal configuration.\n",
    "- Your model must achieve superior **validation F1 score** compared to the naive approach.\n",
    "- You may use different versions of adjacency matrices in your implementation:\n",
    "  - Unmodified adjacency matrices, i.e., `A_train`.\n",
    "  - Self-looped adjacency matrices, i.e., `A_train_self_loops`.\n",
    "  - Normalized adjacency matrices, i.e., `A_train_normalized`.\n",
    "\n",
    "Please see the Coursework Description PDF Question 2.3 for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "class HeteroGCN(nn.Module):\n",
    "    pass\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Convert labels to float if using BCE with sigmoid\n",
    "y_train = y_train.float()  # shape (N,)\n",
    "y_val   = y_val.float()\n",
    "\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "# Build the HeteroGCN model\n",
    "hetero_gcn = HeteroGCN(...)\n",
    "\n",
    "\n",
    "# Train\n",
    "loss_history, _ = train_model(hetero_gcn, y_train, val_model_inputs={...}, y_val=y_val, ...)\n",
    "\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################\n",
    "\n",
    "# Test on validation data\n",
    "val_prec, val_rec, val_f1, _ = evaluate_model(hetero_gcn, y_val, ...)\n",
    "print(f\"[VAL] Prec={val_prec:.4f}, Rec={val_rec:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "val_precs[\"Hetero GCN\"] = val_prec\n",
    "val_recs[\"Hetero GCN\"] = val_rec\n",
    "val_f1s[\"Hetero GCN\"] = val_f1\n",
    "\n",
    "# Plot loss\n",
    "plot_loss(loss_history, title=\"Hetero GNN Approach Training Loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.4 Exploring Attention\n",
    "In this question, you will explore the attention-based aggregation mechanism in GNNs. Attention-based aggregation is explained in [Lecture 3.5](https://www.youtube.com/watch?v=zRmzVkidkqA&list=PLug43ldmRSo14Y_vt7S6vanPGh-JpHR7T&index=14).\n",
    "\n",
    "- Implement an attention-based aggregation mechanism for binary node classification using GCNs.\n",
    "- Limit your model to **two layers**, as in Question 2.2 and Question 2.3\n",
    "- Base your implementation on the naive padding approach introduced in Question 2.2\n",
    "- **Do not** implement a node-type aware special design as in Question 2.3\n",
    "- Your implementation must achieve a better **validation F1 score** compared to the naive GCN in Question 2.2\n",
    "- Determine the optimal hyperparameters for your model, such as hidden dimensions, learning rate, scheduler patience, etc. These hyperparameters do not need to match those of the previous models, as each design has its own optimal configuration. Explain your hyperparameter tuning process **in detail** in your report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "class GraphAttentionNetwork(nn.Module):\n",
    "    pass\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Naively pad\n",
    "X_train, d_max_train = pad_features(A_train, X1_train, X2_train, mapping_train)\n",
    "X_val,   d_max_val   = pad_features(A_val,   X1_val,   X2_val,   mapping_val)\n",
    "\n",
    "print(\"Train adjacency:\", A_train.shape)\n",
    "print(\"Train features after padding:\", X_train.shape, \"with d_max =\", d_max_train)\n",
    "print(\"Val adjacency:\", A_val.shape)\n",
    "print(\"Val features after padding:\", X_val.shape, \"with d_max =\", d_max_val)\n",
    "\n",
    "# Convert labels to float if using BCE with sigmoid\n",
    "y_train = y_train.float()  # shape (N,)\n",
    "y_val   = y_val.float()\n",
    "\n",
    "\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "# Build the model\n",
    "attn_gnn = GraphAttentionNetwork(...)\n",
    "\n",
    "# Train\n",
    "loss_history, _ = train_model(attn_gnn, y_train, val_model_inputs={...}, y_val=y_val, ...)\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################\n",
    "\n",
    "# Test on validation data\n",
    "val_prec, val_rec, val_f1, _ = evaluate_model(attn_gnn, y_val, ...)\n",
    "print(f\"[VAL] Prec={val_prec:.4f}, Rec={val_rec:.4f}, F1={val_f1:.4f}\")\n",
    "\n",
    "val_precs[\"Naive GAT\"] = val_prec\n",
    "val_recs[\"Naive GAT\"] = val_rec\n",
    "val_f1s[\"Naive GAT\"] = val_f1\n",
    "\n",
    "# Plot loss\n",
    "plot_loss(loss_history, title=\"GAT Naive Approach Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.5 Discussion\n",
    "\n",
    "In your report, include the bar plot comparing the **precision, recall, and F1 scores** of the three models, generated using the following function call. Explain your results in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(val_precs, val_recs, val_f1s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl_cw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

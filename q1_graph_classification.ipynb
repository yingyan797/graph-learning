{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Graph Classification\n",
    "\n",
    "In this question we will be exploring graph classification on the dataset provided.\n",
    "\n",
    "Overall, we will explore ways to implement graph level classification using GCNs; explore the given dataset and see how we can improve the GCN & training process overall to get a higher score on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTION & IMPORTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE_NAME = \"q1_graph_classification_main_train.json\"\n",
    "EVAL_FILE_NAME = \"q1_graph_classification_main_eval.json\"\n",
    "\n",
    "TRAIN_DATA_PATH = os.path.join(\"data\",TRAIN_FILE_NAME)\n",
    "EVAL_DATA_PATH = os.path.join(\"data\",EVAL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataset, optimizer, criterion, return_embeddings=False, **kwargs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_embeddings_H1 = []\n",
    "    all_labels_H1 = []\n",
    "\n",
    "    for X, A, label in dataset:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with optional embeddings\n",
    "        if return_embeddings:\n",
    "            output, H1, _ = model(A, X, return_embeddings=True, **kwargs)\n",
    "            all_embeddings_H1.append(H1.detach().cpu().numpy())  # Store H1 embeddings\n",
    "        else:\n",
    "            output = model(A, X, **kwargs)\n",
    "\n",
    "        # Prepare label for BCELoss\n",
    "        adjusted_label = 1 if label == 1 else 0\n",
    "        label_tensor = torch.tensor([float(adjusted_label)], dtype=torch.float)\n",
    "        all_labels_H1.append(label_tensor.detach().cpu().numpy())\n",
    "\n",
    "        # Flatten the output to match the label tensor shape\n",
    "        output_flat = output.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output_flat, label_tensor)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataset)\n",
    "\n",
    "    if return_embeddings:\n",
    "        return average_loss, (all_embeddings_H1,all_labels_H1)\n",
    "    else:\n",
    "        return average_loss\n",
    "\n",
    "\n",
    "# Testing function\n",
    "def test(model, dataset, **kwargs):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, A, label in dataset:\n",
    "            output = model(A, X, **kwargs)\n",
    "\n",
    "            # Convert sigmoid output to binary prediction\n",
    "            predicted = (output >= 0.5).int().item()\n",
    "\n",
    "            # Adjust labels to match binary predictions (0 and 1)\n",
    "            adjusted_label = 1 if label == 1 else 0\n",
    "\n",
    "            true_labels.append(adjusted_label)\n",
    "            predicted_labels.append(predicted)\n",
    "            correct += (predicted == adjusted_label)\n",
    "\n",
    "    accuracy = correct / len(dataset)\n",
    "    return true_labels, predicted_labels, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load the data\n",
    "def create_Adj_matrix(N, edge_index):\n",
    "    \"\"\"Creates the adjacency matrix\"\"\"\n",
    "    A = torch.zeros((N, N), dtype=torch.float)\n",
    "    for idx, jdx in edge_index:\n",
    "        A[idx, jdx] = 1\n",
    "        A[jdx, idx] = 1\n",
    "    return A\n",
    "\n",
    "def read_json_data(file_path, has_label=True):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    graph_data = []\n",
    "    for item in data:\n",
    "        X = torch.tensor(item['features'], dtype=torch.float)\n",
    "        N = len(X)\n",
    "        A = create_Adj_matrix(N, item['edge_index'])\n",
    "        if has_label:\n",
    "            y = torch.tensor(item['label'], dtype=torch.long)\n",
    "        else:\n",
    "            y = None\n",
    "        \n",
    "        graph_data.append((X,A,y))\n",
    "    \n",
    "    return graph_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 - Graph-Level Aggregation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network (GCN)\n",
    "\n",
    "We are going to use a 2-layer GCN for the **binary node classification** task on the heterogeneous graph. Below, you can see the mathematical representation to generate predictions from GCN.\n",
    "\n",
    "The feature update rule for the next layer $ H_{k+1} $ in a graph convolutional network is given by the equation\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "where  $\\tilde{D}$ is the degree matrix with added self-loops, $\\tilde{A}$ is the adjacency matrix with self-loops, $H_k$ are the features from the previous layer, $ \\Omega_k $ is the weight matrix at layer $ k $, $\\Beta_k$ is the bias vector at layer $k$, and $\\sigma $ denotes the activation function.\n",
    "\n",
    "\n",
    "$\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ is the normalized adjacency matrix with self-loops. Below, you need to implement this normalization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to normalise the A matrix\n",
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return D_inv @ A_tilde @ D_inv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1.a - Graph-Level GCN\n",
    "\n",
    "Implement three graph-level aggregation methods: sum, mean, and max. A GCN implemen-\n",
    "tation is provided, and your task is to adapt it into a graph-level GCN by integrating different\n",
    "aggregation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION Q1.1.a\n",
    "\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "class MyGCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of a Graph Convolutional Network (GCN).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
    "        super(MyGCNLayer, self).__init__()\n",
    "        self.use_nonlinearity = use_nonlinearity\n",
    "        self.Omega = nn.Parameter(torch.randn(input_dim, output_dim) * torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim)))\n",
    "        self.beta = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, H_k, A_normalized):\n",
    "        agg = torch.matmul(A_normalized, H_k) # local agg\n",
    "        H_k_next = torch.matmul(agg, self.Omega) + self.beta\n",
    "        return F.relu(H_k_next) if self.use_nonlinearity else H_k_next\n",
    "\n",
    "class MyGraphNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Graph Neural Network model using two layers of Graph Convolutional Network (GCN)\n",
    "    for binary classification. The sigmoid activation is applied in the output layer only if\n",
    "    use_nonlinearity is set to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MyGraphNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Define GCN layers\n",
    "        self.gcn1 = MyGCNLayer(input_dim, hidden_dim, True)\n",
    "        self.gcn2 = MyGCNLayer(hidden_dim, 1, False)\n",
    "\n",
    "    def forward(self, A, X, graph_aggregation_method='mean', **kwargs):\n",
    "        # Pass through GCN layers\n",
    "        H1 = self.gcn1(X, A)\n",
    "        H2 = self.gcn2(H1, A)  # Output shape: (num_nodes, 1)\n",
    "\n",
    "        output = torch.sigmoid(H2)\n",
    "\n",
    "        if torch.isnan(graph_output).any():\n",
    "            graph_output = torch.where(torch.isnan(graph_output), torch.zeros_like(graph_output), graph_output)\n",
    "\n",
    "        if kwargs.get(\"return_embeddings\", None):\n",
    "            return graph_output, H1, H2\n",
    "        else:\n",
    "            return graph_output\n",
    "        \n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your solution runs for all three aggregation methods.\n",
    "\n",
    "Note - the input dataset features have dimensionionality 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A full end-2-end Training script (make sure the below runs)\n",
    "\n",
    "You only need to modify the aggregation parameter in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Basics:\n",
    "# Load data\n",
    "train_data = read_json_data(TRAIN_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "eval_data = read_json_data(EVAL_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "\n",
    "# Initialising the model\n",
    "input_dim = 10\n",
    "hidden_dim = 8\n",
    "model = MyGraphNeuralNetwork(input_dim, hidden_dim)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "random.shuffle(train_data)\n",
    "\n",
    "# Initialize a list to store loss values for visualization\n",
    "loss_values = []\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train without returning embeddings for other epochs\n",
    "    #note train_epoch accepts the additional param graph_aggregation_method and passes it (via kwargs) to the model.forward method.\n",
    "    \n",
    "    # ####################################################\n",
    "    # MODIFY THE CODE BELOW \n",
    "    #  (note only aggregation param - and make sure to run all of them)\n",
    "    # ####################################################    \n",
    "    loss = train_epoch(model, train_data, optimizer, criterion, graph_aggregation_method=\"sum\")\n",
    "\n",
    "\n",
    "    # ####################################################\n",
    "    # END OF MODIFICATION\n",
    "    # ####################################################   \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
    "    loss_values.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "true_labels, predicted_labels, accuracy = test(model, eval_data)\n",
    "\n",
    "# Debug: Print true and predicted labels\n",
    "print(\"True labels:\", true_labels)\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "\n",
    "# Calculate precision, recall (sensitivity), and F1-score\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "# Print out the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1.b. Graph-Level Training\n",
    "\n",
    "Make modifications to the training script to:\n",
    "1. Train the model and record the training loss and evaluation accuracy for each epoch.\n",
    "2. Plot the test f1 for all three aggregation methods.\n",
    "\n",
    "Use the provided functions:\n",
    "\n",
    "• train model\n",
    "\n",
    "• plot training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting:\n",
    "\n",
    "def plot_training_and_validation(training_losses, validation_losses, aggregation=\"mean\", graph1_label=\"Training Accuracy\", graph2_label=\"Validation Accuracy\", x_label=\"Epoch\",title=None):\n",
    "    \"\"\"\n",
    "    Plots the validation accuracy and training loss over epochs.\n",
    "    Args:\n",
    "        validation_losses: List of validation accuracies (or losses)\n",
    "        training_losses: List of training accuracies (or losses)\n",
    "        aggregation: Aggregation method used (default: \"mean\")\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create figure with two y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot training loss on the first y-axis\n",
    "    epochs = range(1, len(training_losses) + 1)\n",
    "    line1 = ax1.plot(epochs, training_losses, 'r-', label=f'{graph1_label}')\n",
    "    ax1.set_xlabel(f\"x_label\")\n",
    "    ax1.set_ylabel(f'{graph1_label}', color='r')\n",
    "    ax1.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    # Plot validation accuracy on the second y-axis\n",
    "    line2 = ax2.plot(epochs, validation_losses, 'b-', label=f'{graph2_label}')\n",
    "    ax2.set_ylabel(f'{graph2_label}', color='b')\n",
    "    ax2.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    # Add title and grid\n",
    "    if not title:\n",
    "        title = f'Training Accuracy and Validation Accuracy Over Time\\nAggregation: {aggregation}'\n",
    "    plt.title(title)\n",
    "\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Add legend\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION Q1.1.b\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################  \n",
    "\n",
    "\n",
    "def train_model(model, optimizer, criterion, train_dataset, validation_dataset, epochs, graph_aggregation_method, verbose):\n",
    "    \"\"\"Trains the model and records loss and validation accuracy\n",
    "    Should return, train_losses, train_accuracy, validation_accuracy\n",
    "    \"\"\"\n",
    "    # INCLUDE THiS STATEMENT IN YOUR CODE\n",
    "    if verbose:\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {loss}, Train Accuracy: {train_acc}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all three aggregation functions here.\n",
    "# Load data\n",
    "train_data = read_json_data(TRAIN_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "eval_data = read_json_data(EVAL_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "\n",
    "# Initialising the model\n",
    "input_dim = 10\n",
    "hidden_dim = 8\n",
    "model = MyGraphNeuralNetwork(input_dim, hidden_dim)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "num_epochs = 20\n",
    "train_losses, train_acc, eval_acc = train_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    train_data, \n",
    "    eval_data, \n",
    "    epochs=num_epochs, \n",
    "    graph_aggregation_method=\"max\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualising\n",
    "plot_training_and_validation(train_acc, eval_acc, aggregation=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1.c - Training vs. Evaluation F1\n",
    "\n",
    "Additionally, compare training F1 scores vs. evaluation F1 scores. What differences do you\n",
    "observe? Which aggregation function performs best and why (sum, mean, or max)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Q1.1.c\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################  \n",
    "\n",
    "train_model()\n",
    "plot_training_and_validation()\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.2 - Analyzing the Dataset\n",
    "\n",
    "You may notice that the model performs worse on the evaluation dataset. The goal of this task\n",
    "is to analyze the dataset and identify potential issues that might affect the model’s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualise graphs:\n",
    "def visualize_graph(G):\n",
    "    \"\"\"Visualize the generated graph.\"\"\"\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(\n",
    "        G, pos, with_labels=True,\n",
    "        node_size=500, font_size=8, font_color=\"white\", edge_color=\"gray\"\n",
    "    )\n",
    "    plt.title(\"Homogenous Graph Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "def visualise_graph_features(features_0, features_1):\n",
    "    \"\"\"plot clusters and their distribution\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Convert to numpy arrays and get the first feature dimension\n",
    "    features_0_np_1 = np.array(features_0)[:, 0]  # Take first feature dimension\n",
    "    features_0_np_2 = np.array(features_0)[:, 1]  # Take second feature dimension\n",
    "    \n",
    "    features_1_np_1 = np.array(features_1)[:, 0]  # Take first feature dimension\n",
    "    features_1_np_2 = np.array(features_1)[:, 1]  # Take first feature dimension\n",
    "   \n",
    "    plt.scatter(features_0_np_1, features_0_np_2, alpha=0.6, label='Class 0', color='red')\n",
    "    plt.scatter(features_1_np_1, features_1_np_2, alpha=0.6, label='Class 1', color='blue')\n",
    "\n",
    "    plt.xlabel('First Feature Dimension')\n",
    "    plt.ylabel('Class') \n",
    "    plt.title('Feature Distribution by Class')\n",
    "    plt.yticks([0, 1], ['Class 0', 'Class 1'])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2.a - Plotting\n",
    "• Plot the topologies of the graphs.\n",
    "\n",
    "• Plot the feature distributions.\n",
    "\n",
    "• Plot the label distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################  \n",
    "def plot_class_distribution(labels):\n",
    "    \"\"\"Plots the distribution of labels\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_graph_topology(A):\n",
    "    \"\"\"Plots the graph topology\"\"\"\n",
    "    pass\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# #################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the above scripts for train and eval. What do you observe?\n",
    "def find_X_A_for_label(list_of_data,label=1):\n",
    "    \"\"\"Label\"\"\"\n",
    "    for X,A,y in list_of_data:\n",
    "        if y==label:\n",
    "            return X,A\n",
    "\n",
    "# Load data\n",
    "train_data = read_json_data(TRAIN_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "eval_data = read_json_data(EVAL_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "\n",
    "# Getting Labels\n",
    "train_labels = [int(y) for _,_,y in train_data]\n",
    "eval_labels = [int(y) for _,_,y in eval_data]\n",
    "\n",
    "# Getting Features\n",
    "train_features_0 = [X.numpy() for X, _, y in train_data if y == 0]\n",
    "train_features_1 = [X.numpy() for X, _, y in train_data if y == 1]\n",
    "\n",
    "eval_features_0 = [X.numpy() for X, _, y in eval_data if y == 0]\n",
    "eval_features_1 = [X.numpy() for X, _, y in eval_data if y == 1]\n",
    "\n",
    "# Find a graph for each of the classes\n",
    "X0_train, A0_train = find_X_A_for_label(train_data, 0)\n",
    "X1_train, A1_train = find_X_A_for_label(train_data, 1)\n",
    "\n",
    "X0_eval,A0_eval = find_X_A_for_label(eval_data, 0)\n",
    "X1_eval, A1_eval = find_X_A_for_label(eval_data, 1)\n",
    "\n",
    "# Plotting Train\n",
    "plot_class_distribution(train_labels)\n",
    "visualise_graph_features(train_features_0, train_features_1)\n",
    "plot_graph_topology(A0_train)\n",
    "plot_graph_topology(A1_train)\n",
    "\n",
    "# Plotting Eval\n",
    "plot_class_distribution(eval_labels)\n",
    "visualise_graph_features(eval_features_0, eval_features_1)\n",
    "plot_graph_topology(A0_eval)\n",
    "plot_graph_topology(A1_eval)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3 - Overcoming Dataset Challenges\n",
    "\n",
    "In this section, you will attempt to address the challenges identified by improving the model or training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3.a - Adapting the GCN\n",
    "\n",
    "Modify your GCN implementation from Q1.1 to accept the number of layers and output dimen-\n",
    "sion (i.e. the graph embedding dimension) as parametes. Experiment with different hyperparam-\n",
    "eters, such as:\n",
    "\n",
    "• Number of layers.\n",
    "\n",
    "• Hidden dimension size.\n",
    "\n",
    "You will implement:\n",
    "\n",
    "• Implement GCN as described above.\n",
    "\n",
    "• Experiment with hyperparameters and report the results and plots.\n",
    "\n",
    "Hint: The ModuleList class might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION Q1.3.a\n",
    "\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "class MyGCNLayer2(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of a Graph Convolutional Network (GCN).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
    "        super(MyGCNLayer, self).__init__()\n",
    "        self.use_nonlinearity = use_nonlinearity\n",
    "        self.Omega = nn.Parameter(torch.randn(input_dim, output_dim) * torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim)))\n",
    "        self.beta = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, H_k, A_normalized):\n",
    "        agg = torch.matmul(A_normalized, H_k) # local agg\n",
    "        H_k_next = torch.matmul(agg, self.Omega) + self.beta\n",
    "        return F.relu(H_k_next) if self.use_nonlinearity else H_k_next\n",
    "\n",
    "class MyGraphNeuralNetwork2(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Graph Neural Network model using two layers of Graph Convolutional Network (GCN)\n",
    "    for binary classification. The sigmoid activation is applied in the output layer only if\n",
    "    use_nonlinearity is set to True.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, output_dim=1):\n",
    "        super(MyGraphNeuralNetwork2, self).__init__()   \n",
    "        pass\n",
    "\n",
    "    def forward(self, A, X, graph_aggregation_method='mean', **kwargs):\n",
    "        # Pass through GCN layers\n",
    "        if kwargs.get(\"return_graph_embedding\", None):\n",
    "            return graph_output, graph_embedding\n",
    "        else:\n",
    "            return graph_output\n",
    "        \n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run different hyper params here\n",
    "\n",
    "# Load data\n",
    "train_data = read_json_data(TRAIN_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "eval_data = read_json_data(EVAL_DATA_PATH, has_label=True) #returns a list of (X,A,y) - features, Adj Matrix, per Graph label\n",
    "\n",
    "# Initialising the model\n",
    "input_dim = 10\n",
    "hidden_dim = 8\n",
    "layers = 2\n",
    "output_dim = 8\n",
    "model = MyGraphNeuralNetwork2(input_dim, hidden_dim, output_dim=output_dim, num_layers = layers)\n",
    "print(model)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Splitting the Data\n",
    "train_data, val_data = split_training_data(train_data,0.8)\n",
    "\n",
    "# TRAIN\n",
    "num_epochs = 20\n",
    "train_losses, train_acc, eval_acc = train_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    train_data, \n",
    "    eval_data, #plotting eval data on purpose to see how eval dataset performance looks like\n",
    "    epochs=num_epochs, \n",
    "    graph_aggregation_method=\"mean\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualising\n",
    "plot_training_and_validation(train_acc, eval_acc, \"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3.b - Improving the Model\n",
    "Identify and implement three different methods to overcome the challenges discovered in\n",
    "Q1.2. Your goal is to achieve the highest possible score. You may experiment with:\n",
    "\n",
    "• Model architecture modifications.\n",
    "\n",
    "• Data preprocessing techniques.\n",
    "\n",
    "• Hyperparameter tuning.\n",
    "\n",
    "• Loss function adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION Q1.3.b\n",
    "\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "\n",
    "# Call this function in the end.\n",
    "plot_training_and_validation(train_acc, eval_acc)\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3.c - Evaluating the Best Model\n",
    "\n",
    "Plot the performance of your best model over 100 epochs or more, averaged over multiple\n",
    "random (i.i.d.) runs (at least 10 runs) to produce a smoothed training and evaluation accuracy\n",
    "curve. \n",
    "\n",
    "Note: Your score will be based on the smoothed curve.\n",
    "\n",
    "Hints:\n",
    "\n",
    "• Set verbose=False in the train model function to suppress excessive logging.\n",
    "\n",
    "• Use np.mean with an axis parameter to compute the average performance over multiple\n",
    "runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION Q1.3.c - Smooth Curve\n",
    "\n",
    "# ####################################################\n",
    "# MODIFY THE CODE BELOW \n",
    "# ####################################################\n",
    "\n",
    "\n",
    "# Call this function in the end (with the averaged values)\n",
    "plot_training_and_validation(averaged_train_acc, averaged_eval_acc)\n",
    "\n",
    "# ####################################################\n",
    "# END OF MODIFICATION\n",
    "# ####################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
